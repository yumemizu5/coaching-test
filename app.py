import streamlit as st
import openai
from gtts import gTTS
from io import BytesIO
import base64
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode
import numpy as np
import speech_recognition as sr

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’è¨­å®š
correct_password = st.secrets.mieai_pw.correct_password

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ã„å ´åˆã®å‡¦ç†
if password == correct_password:

    openai.api_key = st.secrets.OpenAIAPI.openai_api_key

    system_prompt = """
    ã‚ãªãŸã¯å„ªç§€ãªäººã®æ‚©ã¿ã‚’è§£æ±ºã™ã‚‹ã‚³ãƒ¼ãƒã§ã™ã€‚
    æ‚©ã¿ã«å¯¾ã—ã¦è³ªå•ã‚’è¡Œã£ãŸã‚Šã—ã¦æ·±å €ã‚‚è¡Œã£ã¦ãã ã•ã„ã€‚
    æ§˜ã€…ãªæ‰‹æ³•ã‚„ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã§ç›¸è«‡è€…ã®æ‚©ã¿ã®è§£æ±ºæ–¹æ³•ã‚’ææ¡ˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    ã‚ãªãŸã®å½¹å‰²ã¯ã‚³ãƒ¼ãƒãƒ³ã‚°ã‚’è¡Œã†ã“ã¨ãªã®ã§ã€ä¾‹ãˆã°ä»¥ä¸‹ã®ã‚ˆã†ãªæ‚©ã¿ä»¥å¤–ã“ã¨ã‚’èã‹ã‚Œã¦ã‚‚ã€çµ¶å¯¾ã«ç­”ãˆãªã„ã§ãã ã•ã„ã€‚

    * èŠ¸èƒ½äºº
    * æ–™ç†
    * ç§‘å­¦
    * æ­´å²
    """

    # st.session_stateã‚’ä½¿ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚„ã‚Šã¨ã‚Šã‚’ä¿å­˜
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": system_prompt}
        ]

    class AudioProcessor(AudioProcessorBase):
        def __init__(self):
            self.recognizer = sr.Recognizer()
            self.audio_data = None
    
        def recv(self, frame):
            # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å—ã‘å–ã‚‹ãŸã³ã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©
            audio_frame = frame.to_ndarray()
            if self.audio_data is None:
                self.audio_data = np.frombuffer(audio_frame, np.float32)
            else:
                self.audio_data = np.concatenate((self.audio_data, np.frombuffer(audio_frame, np.float32)))
            return frame
    
        def process_audio(self):
            # ååˆ†ãªé•·ã•ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒãŸã¾ã£ãŸã‚‰å‡¦ç†ã™ã‚‹ï¼ˆä¾‹: 5ç§’é–“ï¼‰
            if self.audio_data is not None and len(self.audio_data) > 16000 * 5:  # 5ç§’åˆ†ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿
                # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ãƒˆå½¢å¼ã«å¤‰æ›ã—ã¦å‡¦ç†
                audio_data_bytes = np.int16(self.audio_data).tobytes()
                audio = sr.AudioData(audio_data_bytes, 16000, 2)
    
                try:
                    # Googleã®Web Speech APIã§éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›
                    text = self.recognizer.recognize_google(audio, language="ja-JP")
                    st.session_state["user_input"] = text
                    self.audio_data = None  # å‡¦ç†å¾Œã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ
                except sr.UnknownValueError:
                    st.write("éŸ³å£°ã‚’èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                except sr.RequestError as e:
                    st.write(f"éŸ³å£°èªè­˜ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


    # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¨ã‚„ã‚Šã¨ã‚Šã™ã‚‹é–¢æ•°
    def communicate():
        messages = st.session_state["messages"]

        user_message = {"role": "user", "content": st.session_state["user_input"]}
        messages.append(user_message)

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=messages
        )

        bot_message = response["choices"][0]["message"]
        messages.append(bot_message)

        st.session_state["user_input"] = ""  # å…¥åŠ›æ¬„ã‚’æ¶ˆå»

        # ãƒœãƒƒãƒˆã®å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éŸ³å£°ã«å¤‰æ›
        tts = gTTS(bot_message["content"], lang='ja')  # æ—¥æœ¬èªå¯¾å¿œ
        tts_file = BytesIO()
        tts.write_to_fp(tts_file)
        tts_file.seek(0)

        # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        audio_bytes = tts_file.read()
        audio_base64 = base64.b64encode(audio_bytes).decode()

        # ã‚«ã‚¹ã‚¿ãƒ HTMLã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•å†ç”Ÿ
        audio_html = f"""
            <audio autoplay>
                <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
            </audio>
        """
        st.components.v1.html(audio_html, height=0)

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
        for message in reversed(messages[1:]):  # ç›´è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸Šã«
            speaker = "ğŸ™‚"
            if message["role"] == "assistant":
                speaker = "ğŸ¤–"

            st.write(speaker + ": " + message["content"])
        
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã®æ§‹ç¯‰
    st.title("ã€Œã¿ãˆAiã€ã‚³ãƒ¼ãƒãƒ³ã‚°ãƒœãƒƒãƒˆ")
    st.image("mieai.png")
    st.write("æ‚©ã¿äº‹ã¯ä½•ã§ã™ã‹ï¼Ÿ")

    # éŸ³å£°å…¥åŠ›ã®ãŸã‚ã®UI
    st.write("éŸ³å£°å…¥åŠ›ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ä¸‹ã®ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
    webrtc_ctx = webrtc_streamer(
        key="speech-to-text",
        mode=WebRtcMode.SENDRECV,
        audio_processor_factory=AudioProcessor,
        async_processing=True,
        # éŒ²éŸ³æ™‚é–“ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã®è¨­å®šã‚’è¿½åŠ 
        rtc_configuration={
            "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
        },
        media_stream_constraints={
            "audio": {
                "sampleRate": 16000,  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆã‚’æŒ‡å®šï¼ˆ16kHzï¼‰
                "channelCount": 1     # ãƒ¢ãƒãƒ©ãƒ«éŸ³å£°ã«è¨­å®š
            }
        }
    )

    
    if webrtc_ctx.state.playing:
        audio_processor = webrtc_ctx.audio_processor
        if audio_processor:
            audio_processor.process_audio()


    user_input = st.text_input("æ‚©ã¿äº‹ã‚’ä¸‹ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚", key="user_input", on_change=communicate)

else:
    # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé–“é•ã£ã¦ã„ã‚‹å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    st.write("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¢ãƒ—ãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«æ­£ã—ã„ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
