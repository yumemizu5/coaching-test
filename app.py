import streamlit as st
import openai
from gtts import gTTS
from io import BytesIO
import base64
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode
import numpy as np
import av
import tempfile
from scipy.io.wavfile import write
from audio_recorder_streamlit import audio_recorder
from tempfile import NamedTemporaryFile

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’è¨­å®š
correct_password = st.secrets.mieai_pw.correct_password

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã®å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
password = st.text_input("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", type="password")

# ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ã„å ´åˆã®å‡¦ç†
if password == correct_password:

    openai.api_key = st.secrets.OpenAIAPI.openai_api_key

    system_prompt = """
    ã‚ãªãŸã¯å„ªç§€ãªäººã®æ‚©ã¿ã‚’è§£æ±ºã™ã‚‹ã‚³ãƒ¼ãƒã§ã™ã€‚
    ã“ã‚Œã‹ã‚‰å„é †ç•ªã«è³ªå•ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
    å›ç­”ã«å¯¾ã—ã¦ä¸€ã¤å…±æ„Ÿã‚„æ„è¦‹ã‚’è¨€ã„ãªãŒã‚‰é€²è¡Œã—ã¦ãã ã•ã„ã€‚

    1.æ€§åˆ¥ã‚’èã„ã¦ãã ã•ã„
    2.å‡ºèº«åœ°ã‚’èã„ã¦ãã ã•ã„ã€‚
    3.å¥½ããªé£Ÿã¹ç‰©ã‚’èã„ã¦ãã ã•ã„
    4.å¥½ããªå‹•ç‰©ã¯çŠ¬ã§ã™ã‹ï¼ŸçŒ«ã§ã™ã‹ï¼Ÿ

    """
    
    # st.session_stateã‚’ä½¿ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚„ã‚Šã¨ã‚Šã‚’ä¿å­˜
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "system", "content": system_prompt}
        ]
        # åˆå›ã®ãƒœãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state["messages"].append(
            {"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ã€ã‚³ãƒ¼ãƒã®AIã§ã™ã€‚ãŠè©±ã‚’å§‹ã‚ã¾ã—ã‚‡ã†ã€‚"}
        )

    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©ã™ã‚‹ãƒãƒƒãƒ•ã‚¡ã‚’åˆæœŸåŒ–
    if 'audio_buffer' not in st.session_state:
        st.session_state['audio_buffer'] = []

    class AudioProcessor(AudioProcessorBase):
        def __init__(self) -> None:
            super().__init__()
            st.write("AudioProcessor initialized")
    
        def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
            st.write("Received audio frame")
            audio = frame.to_ndarray().flatten()
            st.write(f"Audio frame length: {len(audio)}")
        # def __init__(self) -> None:
        #     super().__init__()
        #     st.write("AudioProcessor initialized")

        # def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        #     st.write("Received audio frame")
        #     audio = frame.to_ndarray().flatten()
        #     st.session_state['audio_buffer'].extend(audio.tolist())
        #     st.write(f"Audio buffer length: {len(st.session_state['audio_buffer'])}")

            # 3ç§’åˆ†ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿ãŒãŸã¾ã£ãŸã‚‰å‡¦ç†ã™ã‚‹
            if len(st.session_state['audio_buffer']) >= 16000 * 3:
                audio_data = np.array(st.session_state['audio_buffer'], dtype=np.float32)
                st.session_state['audio_buffer'] = []  # ãƒãƒƒãƒ•ã‚¡ã‚’ãƒªã‚»ãƒƒãƒˆ

                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                with tempfile.NamedTemporaryFile(suffix=".wav") as tmp_file:
                    write(tmp_file.name, 16000, audio_data)
                    tmp_file.flush()

                    # Whisper API ã‚’ä½¿ç”¨ã—ã¦éŸ³å£°èªè­˜
                    audio_file = open(tmp_file.name, "rb")
                    try:
                        transcript = openai.Audio.transcribe("whisper-1", audio_file, language="ja")
                        text = transcript["text"]
                        st.session_state["user_input"] = text
                        st.write(f"èªè­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: {text}")
                        communicate()
                    except Exception as e:
                        st.write(f"éŸ³å£°èªè­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

            return frame  # ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãã®ã¾ã¾è¿”ã™



    
    def transcribe_audio_to_text(audio_bytes):
        #openai.api_key = st.secrets.OpenAIAPI.openai_api_key
        with NamedTemporaryFile(delete=True, suffix=".wav") as temp_file:
            temp_file.write(audio_bytes)
            temp_file.flush()
            with open(temp_file.name, "rb") as audio_file:
                response = openai.Audio.transcribe("whisper-1", audio_file, language="ja", temperature=0, top_p=0.1)
        return response["text"]



    

    # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¨ã‚„ã‚Šã¨ã‚Šã™ã‚‹é–¢æ•°
    def communicate():
        messages = st.session_state["messages"]

        user_input = st.session_state.get("user_input", "")
        if user_input:
            user_message = {"role": "user", "content": user_input}
            messages.append(user_message)

            response = openai.ChatCompletion.create(
                model="gpt-4",
                messages=messages
            )

            bot_message = response["choices"][0]["message"]
            messages.append(bot_message)

            st.session_state["user_input"] = ""  # å…¥åŠ›æ¬„ã‚’æ¶ˆå»

            # ãƒœãƒƒãƒˆã®å¿œç­”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éŸ³å£°ã«å¤‰æ›
            tts = gTTS(bot_message["content"], lang='ja')  # æ—¥æœ¬èªå¯¾å¿œ
            tts_file = BytesIO()
            tts.write_to_fp(tts_file)
            tts_file.seek(0)

            # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’base64ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            audio_bytes = tts_file.read()
            audio_base64 = base64.b64encode(audio_bytes).decode()

            # ã‚«ã‚¹ã‚¿ãƒ HTMLã‚’ä½¿ç”¨ã—ã¦è‡ªå‹•å†ç”Ÿ
            audio_html = f"""
                <audio autoplay>
                    <source src="data:audio/mp3;base64,{audio_base64}" type="audio/mp3">
                </audio>
            """
            st.components.v1.html(audio_html, height=0)

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º
        st.write("### ãƒãƒ£ãƒƒãƒˆå±¥æ­´")
        for message in reversed(messages[1:]):  # ç›´è¿‘ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¸Šã«
            speaker = "ğŸ™‚"
            if message["role"] == "assistant":
                speaker = "ğŸ¤–"

            st.write(speaker + ": " + message["content"])

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ã‚¤ã‚¹ã®æ§‹ç¯‰
    st.title("ã€Œã¿ãˆAiã€ã‚³ãƒ¼ãƒãƒ³ã‚°ãƒœãƒƒãƒˆ")
    st.image("mieai.png")
    st.write("ãƒã‚¤ã‚¯ã§ãŠè©±ã—ãã ã•ã„ã€‚")

    # Record audio using Streamlit widget
    audio_bytes = audio_recorder(pause_threshold=30)
    
    # Convert audio to text using OpenAI Whisper API
    if audio_bytes:
        transcript = transcribe_audio_to_text(audio_bytes)
        st.write("Transcribed Text:", transcript)

        # å¤‰æ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«ä¿å­˜
        st.session_state["user_input"] = transcript
        
        # ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã¨ã‚„ã‚Šã¨ã‚Š
        communicate()

    
    # åˆå›ã®ãƒœãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    if "init" not in st.session_state:
        communicate()
        st.session_state["init"] = True

else:
    # ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé–“é•ã£ã¦ã„ã‚‹å ´åˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
    st.write("ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¢ãƒ—ãƒªã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«æ­£ã—ã„ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
